# Implementation Plan: Embedding Pipeline for RAG Retrieval

**Branch**: `002-embedding-pipeline` | **Date**: 2025-12-14 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/002-embedding-pipeline/spec.md`

## Summary

Build a single-file Python pipeline (`main.py`) that crawls a Docusaurus site via sitemap, extracts text content, generates Cohere embeddings, and stores them in Qdrant for RAG retrieval. Uses UV for package management and targets https://spec-kit-hackathon.vercel.app/.

## Technical Context

**Language/Version**: Python 3.10+
**Primary Dependencies**: requests, beautifulsoup4, cohere, qdrant-client, python-dotenv
**Storage**: Qdrant vector database (collection: "rag_embeded")
**Testing**: Manual verification (single-file constraint)
**Target Platform**: Local development / any Python environment
**Project Type**: Single backend script
**Performance Goals**: Process full sitemap in <5 minutes
**Constraints**: Single file only (`main.py`), no additional Python files
**Scale/Scope**: ~25-100 pages from target sitemap

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

| Principle | Status | Notes |
|-----------|--------|-------|
| I. Technical Accuracy | ✅ PASS | Uses official Cohere/Qdrant APIs |
| II. Educational Clarity | ✅ PASS | Clear function names, documented pipeline |
| III. AI-Native Design | ✅ PASS | Content structured for RAG retrieval |
| IV. Reproducibility | ✅ PASS | UV lockfile, .env for config |
| V. Industry Relevance | ✅ PASS | Standard RAG pipeline pattern |
| VI. Open Knowledge | ✅ PASS | Single-file for easy understanding |

**Security Check**:
- ✅ API keys via environment variables (FR-019)
- ✅ No hardcoded secrets
- ✅ .env in .gitignore

## Project Structure

### Documentation (this feature)

```text
specs/002-embedding-pipeline/
├── plan.md              # This file
├── research.md          # Technology decisions
├── data-model.md        # Entity definitions
├── quickstart.md        # Setup guide
├── contracts/           # (empty - no REST API)
└── tasks.md             # Generated by /sp.tasks
```

### Source Code (repository root)

```text
backend/
├── main.py              # ALL pipeline code (single file requirement)
├── .env                 # Environment variables (gitignored)
├── .env.example         # Template for environment setup
├── pyproject.toml       # UV project configuration
├── uv.lock              # Dependency lockfile
└── .python-version      # Python version specification
```

**Structure Decision**: Single backend folder with one Python file per user requirement. No src/, tests/, or additional modules.

## Function Design

All functions in `main.py`:

### 1. `get_all_urls(sitemap_url: str) -> list[str]`

**Purpose**: Fetch and parse sitemap.xml to extract all page URLs

**Implementation**:
```python
def get_all_urls(sitemap_url: str) -> list[str]:
    """Fetch URLs from sitemap.xml."""
    response = requests.get(sitemap_url)
    root = ET.fromstring(response.content)
    namespace = {"ns": "http://www.sitemaps.org/schemas/sitemap/0.9"}
    urls = [loc.text for loc in root.findall(".//ns:loc", namespace)]
    return urls
```

**Maps to**: FR-001, FR-002

---

### 2. `extract_text_from_urls(urls: list[str]) -> list[dict]`

**Purpose**: Fetch each URL and extract main content

**Implementation**:
```python
def extract_text_from_urls(urls: list[str]) -> list[dict]:
    """Extract text content from URLs."""
    documents = []
    for url in urls:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")

        # Remove nav, footer, sidebar
        for tag in soup.find_all(["nav", "footer", "aside", "header"]):
            tag.decompose()

        # Extract main content
        main = soup.find("main") or soup.find("article") or soup.body
        title = soup.title.string if soup.title else url.split("/")[-1]
        content = main.get_text(separator="\n", strip=True) if main else ""

        if len(content) > 100:  # Skip empty pages
            documents.append({
                "url": url,
                "title": title,
                "content": content
            })
    return documents
```

**Maps to**: FR-004, FR-005, FR-006, FR-009

---

### 3. `chunk_text(documents: list[dict], chunk_size: int = 1500, overlap: int = 200) -> list[dict]`

**Purpose**: Split documents into overlapping chunks

**Implementation**:
```python
def chunk_text(documents: list[dict], chunk_size: int = 1500, overlap: int = 200) -> list[dict]:
    """Split documents into chunks with overlap."""
    chunks = []
    for doc in documents:
        text = doc["content"]
        start = 0
        chunk_index = 0
        while start < len(text):
            end = start + chunk_size
            chunk_text = text[start:end]
            chunks.append({
                "text": chunk_text,
                "source_url": doc["url"],
                "title": doc["title"],
                "chunk_index": chunk_index
            })
            start += chunk_size - overlap
            chunk_index += 1
    return chunks
```

**Maps to**: FR-007, FR-008

---

### 4. `embed(chunks: list[dict], cohere_client) -> list[list[float]]`

**Purpose**: Generate Cohere embeddings for all chunks

**Implementation**:
```python
def embed(chunks: list[dict], cohere_client) -> list[list[float]]:
    """Generate embeddings using Cohere."""
    texts = [chunk["text"] for chunk in chunks]

    # Batch in groups of 96 (Cohere limit)
    embeddings = []
    for i in range(0, len(texts), 96):
        batch = texts[i:i+96]
        response = cohere_client.embed(
            texts=batch,
            model="embed-english-v3.0",
            input_type="search_document"
        )
        embeddings.extend(response.embeddings)
    return embeddings
```

**Maps to**: FR-010, FR-011, FR-013

---

### 5. `create_collection(qdrant_client, collection_name: str = "rag_embeded")`

**Purpose**: Create Qdrant collection if not exists

**Implementation**:
```python
def create_collection(qdrant_client, collection_name: str = "rag_embeded"):
    """Create Qdrant collection with correct vector config."""
    from qdrant_client.models import Distance, VectorParams

    collections = qdrant_client.get_collections().collections
    if not any(c.name == collection_name for c in collections):
        qdrant_client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
        )
```

**Maps to**: FR-014, FR-015

---

### 6. `save_chunk_to_qdrant(qdrant_client, chunks: list[dict], embeddings: list[list[float]], collection_name: str = "rag_embeded")`

**Purpose**: Upsert embeddings with metadata to Qdrant

**Implementation**:
```python
def save_chunk_to_qdrant(qdrant_client, chunks: list[dict], embeddings: list[list[float]], collection_name: str = "rag_embeded"):
    """Save embeddings to Qdrant with metadata."""
    from qdrant_client.models import PointStruct
    import hashlib
    import uuid

    points = []
    for chunk, embedding in zip(chunks, embeddings):
        # Deterministic ID for deduplication
        id_content = f"{chunk['source_url']}:{chunk['chunk_index']}"
        point_id = str(uuid.UUID(bytes=hashlib.md5(id_content.encode()).digest()))

        points.append(PointStruct(
            id=point_id,
            vector=embedding,
            payload={
                "source_url": chunk["source_url"],
                "title": chunk["title"],
                "chunk_text": chunk["text"],
                "chunk_index": chunk["chunk_index"]
            }
        ))

    # Batch upsert
    qdrant_client.upsert(collection_name=collection_name, points=points)
```

**Maps to**: FR-016, FR-017, FR-018

---

### 7. `main()`

**Purpose**: Orchestrate the full pipeline

**Implementation**:
```python
def main():
    """Execute full embedding pipeline."""
    import os
    from dotenv import load_dotenv
    import cohere
    from qdrant_client import QdrantClient

    load_dotenv()

    # Initialize clients
    cohere_client = cohere.Client(os.getenv("COHERE_API_KEY"))
    qdrant_client = QdrantClient(
        url=os.getenv("QDRANT_URL"),
        api_key=os.getenv("QDRANT_API_KEY")
    )

    sitemap_url = "https://spec-kit-hackathon.vercel.app/sitemap.xml"

    print("[INFO] Fetching URLs from sitemap...")
    urls = get_all_urls(sitemap_url)
    print(f"[INFO] Found {len(urls)} URLs")

    print("[INFO] Extracting text from URLs...")
    documents = extract_text_from_urls(urls)
    print(f"[INFO] Extracted {len(documents)} documents")

    print("[INFO] Chunking documents...")
    chunks = chunk_text(documents)
    print(f"[INFO] Created {len(chunks)} chunks")

    print("[INFO] Generating embeddings...")
    embeddings = embed(chunks, cohere_client)
    print(f"[INFO] Generated {len(embeddings)} embeddings")

    print("[INFO] Creating Qdrant collection 'rag_embeded'...")
    create_collection(qdrant_client)

    print("[INFO] Saving to Qdrant...")
    save_chunk_to_qdrant(qdrant_client, chunks, embeddings)

    print("[INFO] Pipeline complete!")
    print(f"[INFO] Stats: {len(documents)} pages, {len(chunks)} chunks, {len(embeddings)} vectors stored")

if __name__ == "__main__":
    main()
```

**Maps to**: FR-020, FR-021

## Implementation Sequence

```text
Step 1: Project Setup
├── Create backend/ folder
├── Initialize UV project (uv init)
├── Add dependencies (uv add ...)
└── Create .env.example

Step 2: URL Ingestion
├── Implement get_all_urls()
└── Test: Verify URLs extracted from sitemap

Step 3: Text Extraction
├── Implement extract_text_from_urls()
└── Test: Verify content extraction from sample URL

Step 4: Chunking
├── Implement chunk_text()
└── Test: Verify chunk sizes and overlap

Step 5: Embedding
├── Implement embed()
└── Test: Verify Cohere API returns vectors

Step 6: Qdrant Storage
├── Implement create_collection()
├── Implement save_chunk_to_qdrant()
└── Test: Verify vectors in Qdrant

Step 7: Pipeline Integration
├── Implement main()
├── Add logging throughout
└── Test: Full pipeline execution
```

## Dependency Sequence

```text
UV Setup ─────────────────────────────┐
                                      │
URL Ingestion ◄───────────────────────┤
      │                               │
      ▼                               │
Text Extraction ◄─────────────────────┤
      │                               │
      ▼                               │
Chunking ◄────────────────────────────┤
      │                               │
      ▼                               │
Embedding ◄───────────────────────────┤
      │                               │
      ▼                               │
Qdrant Collection ◄───────────────────┤
      │                               │
      ▼                               │
Qdrant Upsert ◄───────────────────────┘
```

## Design Decisions

| Decision | Rationale |
|----------|-----------|
| Single file (`main.py`) | User requirement for simplicity |
| UV package manager | Fast, modern Python packaging |
| Character-based chunking | No extra dependencies (tiktoken) |
| Deterministic IDs | Enables idempotent re-indexing |
| Batch embedding (96) | Cohere API limit optimization |
| Hardcoded sitemap URL | User-specified target site |
| Collection name "rag_embeded" | User-specified requirement |

## Complexity Tracking

> No violations - single-file architecture is the simplest possible design.

| Aspect | Complexity | Justification |
|--------|------------|---------------|
| Files | 1 Python file | User requirement |
| Dependencies | 5 packages | Minimum needed |
| Functions | 7 functions | One per pipeline stage |
| External services | 2 (Cohere, Qdrant) | Core to RAG pipeline |

## Next Steps

Run `/sp.tasks` to generate the task breakdown for implementation.
